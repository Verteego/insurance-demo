{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0,
  "cells": [
    {
      "cell_type": "markdown",
      "source": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.preprocessing import StandardScaler",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "input_data = pd.read_csv('../input/caravan-insurance-challenge.csv')\ninput_data.head()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "data_no_origin_train = input_data[input_data['ORIGIN']=='train'].drop(['ORIGIN'], axis=1)\ndata_no_origin_train.head()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "data_no_origin_test = input_data[input_data['ORIGIN']=='test'].drop(['ORIGIN'], axis=1)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "input_train, input_cv = train_test_split(data_no_origin_train, test_size=0.30)\ninput_test = data_no_origin_test",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "input_train.describe()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "fig = plt.figure(figsize=(10,10))\n\n# Tells the total count of different values in CARAVAN\nplt.subplot(3,1,1)\ninput_train['CARAVAN'].value_counts().plot(kind='bar', title='Classifying CARAVAN', color='steelblue', grid=True)\n\n# Tells the total count of different values in customer subtype\nplt.subplot(3,1,2)\ninput_train['MOSTYPE'].value_counts().plot(kind='bar', align='center', title='Classifying customer subtypes', color='steelblue', grid=True)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Plotting the dependency of prefering caravan policy based on category subtype",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "categorysubtype_caravan = pd.crosstab(input_train['MOSTYPE'], input_train['CARAVAN'])\ncategorysubtype_caravan_pct = categorysubtype_caravan.div(categorysubtype_caravan.sum(1).astype(float), axis=0)\ncategorysubtype_caravan_pct.plot(figsize= (8,5), kind='bar', stacked=True, color=['steelblue', 'springgreen'], title='category type vs Caravan', grid=True)\nplt.xlabel('Category subtype')\nplt.ylabel('Caravan or not')",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Plotting the dependency of prefering caravan policy based on age",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "input_train['MGEMLEEF'].hist(figsize=(5,3), fc='steelblue', grid=True)\nplt.xlabel('age')\nplt.ylabel('count')",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "age_caravan = pd.crosstab(input_train['MGEMLEEF'], input_train['CARAVAN'])\nage_caravan_pct = age_caravan.div(age_caravan.sum(1).astype(float),axis=0)\nage_caravan_pct.plot(figsize=(5,3), kind='bar', stacked=True, color=['steelblue', 'springgreen'], title='dependency of caravan on age groups', grid=True)\nplt.xlabel('age groups')\nplt.ylabel('Caravan')",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "We can verify that age group 1: 20-30yrs don't prefer the caravan policy. thus age, Subtype are important features  for correct classification.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "### Plotting the dependency of prefering caravan policy based on Customer type",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "input_train['MOSHOOFD'].value_counts().plot(kind='bar', color='steelblue', grid=True)\nplt.xlabel('Customer Main Types')\nplt.ylabel('count')",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "cust_type_caravan = pd.crosstab(input_train['MOSHOOFD'], input_train['CARAVAN'])\ncust_type_caravan_pct = cust_type_caravan.div(cust_type_caravan.sum(1).astype(float), axis=0)\ncust_type_caravan_pct.plot(kind='bar', stacked=True, color = ['steelblue', 'springgreen'])\nplt.xlabel('customer types')\nplt.ylabel('caravan')",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "train_data = input_train.values\ntrain_data",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from sklearn.ensemble import RandomForestClassifier\n\nclf = RandomForestClassifier(n_estimators=100)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Training data features, skip the first column 'Survived'\ntrain_features = train_data[:, :-1]\n\n# 'Survived' column values\ntrain_target = train_data[:, -1]\n\n# Fit the model to our training data\nclf = clf.fit(train_features, train_target)\nscore = clf.score(train_features, train_target)\n\"Mean accuracy of Random Forest: {0}\".format(score)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "cv_data = input_cv.values\ncv_data",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Training data features, skip the last column 'CARAVAN'\ncv_features = cv_data[:, :-1]\n\n# 'caravan' column values\ncv_target = cv_data[:, -1]",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "cv_predictions = clf.predict(cv_features)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from sklearn.metrics import accuracy_score\nprint (\"Accuracy = %.2f\" % (accuracy_score(cv_target, cv_predictions)))",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Confusion matrix and confusion tables: \nThe columns represent the actual class and the rows represent the predicted class. Lets evaluate performance: ",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\n\ndef draw_confusion_matrices(confusion_matricies,class_names):\n    class_names = class_names.tolist()\n    for cm in confusion_matrices:\n        classifier, cm = cm[0], cm[1]\n        print(cm)\n        \n        fig = plt.figure()\n        ax = fig.add_subplot(111)\n        cax = ax.matshow(cm)\n        plt.title('Confusion matrix for %s' % classifier)\n        fig.colorbar(cax)\n        ax.set_xticklabels([''] + class_names)\n        ax.set_yticklabels([''] + class_names)\n        plt.xlabel('Predicted')\n        plt.ylabel('True')\n        plt.show()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "class_names = np.unique(np.array(cv_target))\nconfusion_matrices = [\n    #( \"Support Vector Machines\", confusion_matrix(y,run_cv(X,y,SVC)) ),\n    ( \"Random Forest\", confusion_matrix(cv_target, cv_predictions)),\n    #( \"K-Nearest-Neighbors\", confusion_matrix(y,run_cv(X,y,KNN)) ),\n    #( \"Gradient Boosting Classifier\", confusion_matrix(y,run_cv(X,y,GBC)) ),\n    #( \"Logisitic Regression\", confusion_matrix(y,run_cv(X,y,LR)) )\n]\ndraw_confusion_matrices(confusion_matrices,class_names)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "forest = RandomForestClassifier()\nforest_fit = forest.fit(train_features, train_target)\nforest_predictions = forest_fit.predict(cv_features)\n\nimportances = forest_fit.feature_importances_[:10]\nstd = np.std([tree.feature_importances_ for tree in forest.estimators_],\n             axis=0)\nindices = np.argsort(importances)[::-1]\n\n# Print the feature ranking\nprint(\"Feature ranking:\")\n\nfeatures = input_train.columns\n\nfor f in range(10):\n    print(\"%d. %s (%f)\" % (f + 1, features[f], importances[indices[f]]))\n\n# Plot the feature importances of the forest\n#import pylab as pl\nplt.figure()\nplt.title(\"Feature importances\")\nplt.bar(range(10), importances[indices], yerr=std[indices], color=\"r\", align=\"center\")\nplt.xticks(range(10), indices)\nplt.xlim([-1, 10])\nplt.show()",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "test_data = input_test.values\ntest_data",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Training data features, skip the last column 'CARAVAN'\ntest_features = test_data[:, :-1]\n\n# 'caravan' column values\ntest_target = test_data[:, -1]",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "test_predictions = clf.predict(test_features)",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from sklearn.metrics import accuracy_score\nprint (\"Accuracy = %.3f\" % (accuracy_score(test_target, test_predictions)))",
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": null,
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    }
  ]
}